{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":95051,"databundleVersionId":11335067,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":21.45991,"end_time":"2025-03-09T03:33:01.442545","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-03-09T03:32:39.982635","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Step 1:\n\n<font size = '5'>The code starts by importing necessary libraries for data manipulation, visualization, and machine learning. It then loads three datasets: a sample submission file (for Kaggle competition format), a test dataset, and a training dataset.</font>","metadata":{"papermill":{"duration":0.003561,"end_time":"2025-03-09T03:32:42.795980","exception":false,"start_time":"2025-03-09T03:32:42.792419","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Import standard libraries (pandas, mathplotlib, seaborn)\n# Import specific tools from libraries: sklearn (score validation & tools, regression models)\n# xgboost (gradient boosting); scipy(parameters for iteration)\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport kagglehub\nimport numpy as np\nimport math\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\nfrom sklearn.metrics import mean_squared_error, r2_score, make_scorer\nfrom scipy.stats import randint, uniform\n\n# Load datasets\ncsv_file = \"/kaggle/input/ai-x-data-supervised-pillar-spring-2025/sample_submission.csv\"\ndf = pd.read_csv(csv_file)\nprint(df.head())\n\ntest_df = pd.read_csv(\"/kaggle/input/ai-x-data-supervised-pillar-spring-2025/test.csv\")\nprint(dtest.head())\n\ntrain_df = pd.read_csv(\"/kaggle/input/ai-x-data-supervised-pillar-spring-2025/train.csv\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-03-10T19:27:27.197802Z","iopub.status.idle":"2025-03-10T19:27:27.198297Z","shell.execute_reply":"2025-03-10T19:27:27.198087Z"},"papermill":{"duration":5.574284,"end_time":"2025-03-09T03:32:48.373125","exception":false,"start_time":"2025-03-09T03:32:42.798841","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 2:\n\n<font size='5'>This section handles missing values in both test and training datasets. For numeric columns, missing values are replaced with the median. For categorical columns (object type), missing values are replaced with the mode (most frequent value). Later in the code, a more robust approach is taken by first converting all columns to numeric where possible and then filling missing values.</font>","metadata":{"papermill":{"duration":0.002444,"end_time":"2025-03-09T03:32:48.378625","exception":false,"start_time":"2025-03-09T03:32:48.376181","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Forces numerical values for all non objects, converts objects to NaN values\ntrain_df = train_df.apply(pd.to_numeric, errors='coerce')\ntest_df = test_df.apply(pd.to_numeric, errors='coerce')\n\n# Fill NaN values with medium\ntrain_df.fillna(train_df.median(numeric_only=True), inplace=True)\ntest_df.fillna(test_df.median(numeric_only=True), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2025-03-10T19:27:27.199071Z","iopub.status.idle":"2025-03-10T19:27:27.199430Z","shell.execute_reply":"2025-03-10T19:27:27.199298Z"},"papermill":{"duration":0.076437,"end_time":"2025-03-09T03:32:48.457808","exception":false,"start_time":"2025-03-09T03:32:48.381371","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 3:\n<font size = '5'>This section visualizes the data for use to understand and see the relationships between variables. It creates scatter plots to see how average disbursed loans vary by school and school type, boxplots to compare loan distributions across school types, and calculates summary statistics for each school type. A histogram also shows the overall distribution of loan amounts.<font>","metadata":{"papermill":{"duration":0.002656,"end_time":"2025-03-09T03:32:48.464141","exception":false,"start_time":"2025-03-09T03:32:48.461485","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Scatter plot: Avg disbursed loan by school\nplt.figure(figsize=(12, 6))\nsns.scatterplot(x=dtrain[\"school\"], y=dtrain[\"avg_disbursed_loan\"], hue=dtrain[\"school_type\"])\nplt.title(\"Average Disbursed Loan by School\")\nplt.xlabel(\"School\")\nplt.ylabel(\"Avg Disbursed Loan\")\nplt.xticks(rotation=90)\nplt.show()\n\n#Boxplot the features and show the avg disbursed loan by school type\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=dtrain[\"school_type\"], y=dtrain[\"avg_disbursed_loan\"])\nplt.title(\"Average Disbursed Loan by School Type\")\nplt.xlabel(\"School Type\")\nplt.ylabel(\"Avg Disbursed Loan\")\nplt.show()\n\n# Summary statistics for avg disbursed loan by school type\nsummary_stats = dtrain.groupby(\"school_type\")[\"avg_disbursed_loan\"].describe()\nprint(\"Summary Statistics by School Type:\\n\", summary_stats)\n\n# Histogram of loan distribution\nsns.histplot(train_df[\"avg_disbursed_loan\"], bins=30, kde=True)\nplt.title(\"Loan Distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-03-10T19:27:27.200205Z","iopub.status.idle":"2025-03-10T19:27:27.200611Z","shell.execute_reply":"2025-03-10T19:27:27.200467Z"},"papermill":{"duration":5.482144,"end_time":"2025-03-09T03:32:53.950050","exception":false,"start_time":"2025-03-09T03:32:48.467906","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 4: Feature Creation\n\n<font size = '5'>This function creates new features from existing ones to potentially improve the model's performance. It creates combinations of tuition and career pay estimates, calculates the ratio of tuition to household income, and combines room/board price with enrollment. The code then prepares the feature matrix (X) and target variable (y) for model training. </font>\n","metadata":{"papermill":{"duration":0.012002,"end_time":"2025-03-09T03:32:53.974440","exception":false,"start_time":"2025-03-09T03:32:53.962438","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#Function for making new features, when we call to it\ndef engineer_features(df):\n    #Define list of columns that need to be numeric\n    numeric_cols = [\"in_state_tuition\", \"early_career_pay_estimate\", \"mid_career_pay_estimate\",\n                    \"zip_median_household_income\", \"room_and_board_price\", \"total_enrollment\"]\n\n    #Make sure the columns are numeric and make sure they are numeric\n    for col in numeric_cols:\n        if col in df.columns:\n            #Convert all the columns to numeric ones, if not make it none\n            df[col] = pd.to_numeric(df[col], errors='coerce')\n\n    # Allow new features to be made from what we currently have in our columns\n    #For example, if we have in_state_tuition and early_career_pay_estimate then we can make tuition_earlyCareer\n    if all(col in df.columns for col in [\"in_state_tuition\", \"early_career_pay_estimate\"]):\n        df[\"tuition_earlyCareer\"] = df[\"in_state_tuition\"] * df[\"early_career_pay_estimate\"]\n    if all(col in df.columns for col in [\"in_state_tuition\", \"mid_career_pay_estimate\"]):\n        df[\"tuition_midCareer\"] = df[\"in_state_tuition\"] * df[\"mid_career_pay_estimate\"]\n    if all(col in df.columns for col in [\"in_state_tuition\", \"zip_median_household_income\"]):\n        df[\"tuition_income_ratio\"] = df[\"in_state_tuition\"] / (df[\"zip_median_household_income\"] + 1)\n    if all(col in df.columns for col in [\"room_and_board_price\", \"total_enrollment\"]):\n        df[\"room_board_enrollment\"] = df[\"room_and_board_price\"] * df[\"total_enrollment\"]\n\n    #Now we return the dataframe with the new features that we made\n    return df\n\n# Prepare the features and the target from the training data\nX = train_df.drop([\"avg_disbursed_loan\", \"id\"], axis=1, errors=\"ignore\")\ny = train_df[\"avg_disbursed_loan\"].values\n\n# Apply our function to make new features for the training set\nX = engineer_features(X)\n#Prepare the test set and drop ID\nX_test = test_df.drop([\"id\"], axis=1, errors=\"ignore\")\n#This is the features engineered for the test set\nX_test = engineer_features(X_test)","metadata":{"execution":{"iopub.status.busy":"2025-03-10T19:27:27.201489Z","iopub.status.idle":"2025-03-10T19:27:27.201977Z","shell.execute_reply":"2025-03-10T19:27:27.201753Z"},"papermill":{"duration":0.034778,"end_time":"2025-03-09T03:32:54.021317","exception":false,"start_time":"2025-03-09T03:32:53.986539","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 5: Model Training and Hyperparameter Tuning\n\n<font size = '5'>This segment splits the data into training and validation sets, creates a custom MSE (Root Mean Squared Error) scoring function, and performs hyperparameter tuning for an XGBoost regression model using RandomizedSearchCV. The hyperparameter search explores different tree depths, learning rates, and sampling strategies to find the best combination.</font>","metadata":{"papermill":{"duration":0.012906,"end_time":"2025-03-09T03:32:54.046838","exception":false,"start_time":"2025-03-09T03:32:54.033932","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Train/validation split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Critical fix: Reset indexes after split and \nX_train.reset_index(drop=True, inplace=True)\nX_val.reset_index(drop=True, inplace=True)\ny_train = pd.Series(y_train).reset_index(drop=True)\ny_val = pd.Series(y_val).reset_index(drop=True)\n\n# MSE Scorer\ndef mse_metric(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred)\n\n# RMSE Scorer\ndef rmse_metric(y_true, y_pred):\n    return math.sqrt(mean_squared_error(y_true, y_pred))\n    \nmse_scorer = make_scorer(mse_metric, greater_is_better=False)\nrmse_scorer = make_scorer(rmse_metric, greater_is_better=False)\n\n# XGBoost + Randomized Search\nxgb_model = XGBRegressor(random_state=42, tree_method=\"hist\")\n\nparam_distributions = {\n    \"n_estimators\": randint(100, 300),\n    \"max_depth\": randint(3, 7),\n    \"learning_rate\": uniform(0.05, 0.15),\n    \"subsample\": uniform(0.7, 0.3),\n    \"colsample_bytree\": uniform(0.7, 0.3)\n}\n\ncv = KFold(n_splits=3, shuffle=True, random_state=42)\n\nrandom_search = RandomizedSearchCV(\n    estimator=xgb_model,\n    param_distributions=param_distributions,\n    n_iter=10,\n    scoring=rmse_scorer,\n    cv=cv,\n    n_jobs=-1,\n    verbose=1,\n    random_state=42\n)\n\n#We do not want to overfit the model so we are setting paramters for early stopping by using an evaluation set and how many rounds if it does not improve\nfit_params = {\n    \"eval_set\": [(X_val, y_val)],\n    \"early_stopping_rounds\": 30,\n    \"verbose\": False\n}\n\n\n#FineTuning\n#Fit model using RandSearchCV which samples the parameters we have set before\nrandom_search.fit(X_train, y_train, **fit_params)\n\n#Find the best parameter using our search \n#Give us the best model that we had with the different parameters according to our RMSE\nprint(\"\\nBest Params:\", random_search.best_params_)\n#Hold the best model \nbest_xgb = random_search.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2025-03-10T19:27:27.202738Z","iopub.status.idle":"2025-03-10T19:27:27.203154Z","shell.execute_reply":"2025-03-10T19:27:27.203006Z"},"papermill":{"duration":4.420639,"end_time":"2025-03-09T03:32:58.480720","exception":false,"start_time":"2025-03-09T03:32:54.060081","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 6: Model Evaluation\n\n<font size = '5'>The best XGBoost model from the hyperparameter search is evaluated on the validation set. Two metrics are reported: RMSE (lower is better), MSE (lower is better), and R² (higher is better). RMSE measures the average prediction error in the original units, while R² indicates the proportion of variance explained by the model.</font>","metadata":{"papermill":{"duration":0.011909,"end_time":"2025-03-09T03:32:58.505068","exception":false,"start_time":"2025-03-09T03:32:58.493159","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Validation\ny_val_pred = best_xgb.predict(X_val) #random search funtion from XGB\n#20% of original data for testing\n\nval_rmse = rmse_metric(y_val, y_val_pred)\nval_r2 = r2_score(y_val, y_val_pred)\n\nprint(f\"\\nValidation RMSE: {val_rmse:.3f}\")\nprint(f\"Validation R^2: {val_r2:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2025-03-10T19:27:27.204005Z","iopub.status.idle":"2025-03-10T19:27:27.204449Z","shell.execute_reply":"2025-03-10T19:27:27.204256Z"},"papermill":{"duration":0.026796,"end_time":"2025-03-09T03:32:58.543871","exception":false,"start_time":"2025-03-09T03:32:58.517075","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 7: Feature Analysis\n\n<font size =5>This section analyzes the importance of each feature in the trained XGBoost model. Features with very low importance (< 0.0001) are identified and optionally removed from the dataset to simplify the model.</font>\n","metadata":{"papermill":{"duration":0.012073,"end_time":"2025-03-09T03:32:58.568818","exception":false,"start_time":"2025-03-09T03:32:58.556745","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Feature importances\nimportances = best_xgb.feature_importances_\nimp_series = pd.Series(importances, index=X_train.columns).sort_values(ascending=False)\nprint(\"\\nTop 20 Features by Importance:\")\nprint(imp_series.head(20))\n\n#best_xgb.feature_importances_ retrieves the importance scores\n#of each feature.\n#These scores indicate how much each feature contributes \n#to the model’s predictions.\n#Higher values = more important features; lower values = less useful features.\n\n# Optional: drop low-importance features\nlow_importance = imp_series[imp_series < 1e-4].index\nif len(low_importance) > 0:\n    print(\"\\nDropping these near-zero-importance features:\", list(low_importance))\n    X_train = X_train.drop(columns=low_importance, errors=\"ignore\")\n    X_val = X_val.drop(columns=low_importance, errors=\"ignore\")\n    X_test = X_test.drop(columns=low_importance, errors=\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2025-03-10T19:27:27.205450Z","iopub.status.idle":"2025-03-10T19:27:27.205940Z","shell.execute_reply":"2025-03-10T19:27:27.205692Z"},"papermill":{"duration":0.025319,"end_time":"2025-03-09T03:32:58.606175","exception":false,"start_time":"2025-03-09T03:32:58.580856","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 8: Final Model Training and Prediction\n\n<font size = '5'>This final section combines the training and validation sets to train the model on all available data. It also creates an ensemble model by averaging predictions from XGBoost and Random Forest models, which often yields better performance than either model alone. The ensemble is evaluated on the validation set, and then used to generate predictions for the test set. Finally, a submission file is created for the Kaggle competition.</font>\n","metadata":{"papermill":{"duration":0.014839,"end_time":"2025-03-09T03:32:58.633457","exception":false,"start_time":"2025-03-09T03:32:58.618618","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Retrain on all data\nX_train.reset_index(drop=True, inplace=True)\nX_val.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\ny_val.reset_index(drop=True, inplace=True)\n\nX_combined = pd.concat([X_train, X_val], ignore_index=True)\ny_combined = pd.concat([y_train, y_val], ignore_index=True)\n\nbest_xgb.fit(\n    X_combined, \n    y_combined,\n    early_stopping_rounds=30,\n    eval_set=[(X_combined, y_combined)],\n    verbose=False\n)\n\n# Optional ensemble\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X_combined, y_combined)\n\ny_val_pred_xgb = best_xgb.predict(X_val)\ny_val_pred_rf = rf.predict(X_val)\ny_val_pred_ensemble = 0.5 * y_val_pred_xgb + 0.5 * y_val_pred_rf\n\nensemble_rmse = rmse_metric(y_val, y_val_pred_ensemble)\n#measures the average magnitude of errors \n#(how far predictions are from actual values) \n#in the same unit as the target variable.\nensemble_r2 = r2_score(y_val, y_val_pred_ensemble)\n\nprint(f\"\\nEnsemble RMSE: {ensemble_rmse:.3f}\")\nprint(f\"Ensemble R^2: {ensemble_r2:.3f}\")\n\n# Final predictions\ny_test_pred_xgb = best_xgb.predict(X_test)\ny_test_pred_rf = rf.predict(X_test)\ny_test_pred_ensemble = 0.5 * y_test_pred_xgb + 0.5 * y_test_pred_rf\n\nsubmission = pd.DataFrame({\n    \"id\": test_df[\"id\"],\n    \"avg_disbursed_loan\": y_test_pred_ensemble\n})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Saved submission.csv\")\nprint(submission)","metadata":{"execution":{"iopub.status.busy":"2025-03-10T19:27:27.206957Z","iopub.status.idle":"2025-03-10T19:27:27.207395Z","shell.execute_reply":"2025-03-10T19:27:27.207205Z"},"papermill":{"duration":1.757727,"end_time":"2025-03-09T03:33:00.405715","exception":false,"start_time":"2025-03-09T03:32:58.647988","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}